# EAI Lab5 Report - 壓縮模型的網頁部署

**學號：N26135011**

---

## Topic 1 & 2：實作說明

這次作業主要是把老師給的 ResNet18 模型轉換成 ONNX 格式，然後做 INT8 量化，最後用 Gradio 建立網頁介面來展示。

### 實作流程

1. **定義 ResNet18 模型架構**：參考 `resnet18.py` 寫出 `BasicBlock` 和 `ResNet18` 類別
2. **載入權重**：用 `load_state_dict()` 載入老師給的 `best_model.pth`
3. **匯出 ONNX**：用 `torch.onnx.export()` 把模型轉成 ONNX 格式
4. **INT8 量化**：用 `onnxruntime.quantization.quantize_static()` 做靜態量化
5. **Gradio 部署**：建立網頁介面讓使用者可以上傳圖片測試

---

## Topic 3：Report

### 3.1 跨裝置測試截圖

我用手機連到 Gradio 產生的公開網址，測試了 3 張不同的圖片：

#### 測試 1：鹿 (Deer)

<!-- 請在這裡插入鹿的測試截圖 -->
[圖片位置 - 鹿的測試結果]

**測試結果：**
- FP32 預測：deer (83%)
- INT8 預測：deer (73%)
- 兩個模型都正確辨識出是鹿

#### 測試 2：飛機 (Plane)

<!-- 請在這裡插入飛機的測試截圖 -->
[圖片位置 - 飛機的測試結果]

**測試結果：**
- FP32 預測：plane (84%)
- INT8 預測：plane (83%)
- 兩個模型都正確辨識出是飛機，信心度也很接近

#### 測試 3：貓咪玩偶

<!-- 請在這裡插入貓咪玩偶的測試截圖 -->
[圖片位置 - 貓咪玩偶的測試結果]

**測試結果：**
- FP32 預測：bird (38%)
- INT8 預測：bird (37%)
- 這張比較難辨識，因為是玩偶不是真的動物，而且被壓縮成 32x32 後細節都不見了

---

### 3.2 FP32 vs INT8 分析

#### 模型檔案大小比較

| 模型 | 檔案大小 | 壓縮比 |
|------|----------|--------|
| FP32 (N26135011_FP32.onnx) | 43 MB | - |
| INT8 (N26135011_INT8.onnx) | 11 MB | 約 **3.9 倍** |

INT8 模型比 FP32 小了將近 4 倍，這是因為 INT8 用 8-bit 整數存權重，而 FP32 用 32-bit 浮點數。

#### 推論時間比較

| 測試圖片 | FP32 時間 | INT8 時間 | 加速比 |
|----------|-----------|-----------|--------|
| 鹿 | 42.26 ms | 37.85 ms | 1.12x |
| 飛機 | 27.51 ms | 26.36 ms | 1.04x |
| 貓咪玩偶 | 23.93 ms | 12.59 ms | 1.90x |

從測試結果來看，INT8 模型的推論速度通常比 FP32 快，加速效果大約在 1.04x ~ 1.90x 之間。

在 Notebook 裡面用假資料測試時的結果更明顯：
- FP32 平均：7.04 ms
- INT8 平均：1.60 ms
- 加速比：約 **4.4 倍**

#### 預測結果比較

| 測試圖片 | FP32 Top-1 | INT8 Top-1 | 是否一致 |
|----------|------------|------------|----------|
| 鹿 | deer (83%) | deer (73%) | ✓ |
| 飛機 | plane (84%) | plane (83%) | ✓ |
| 貓咪玩偶 | bird (38%) | bird (37%) | ✓ |

三張測試圖片的 Top-1 預測結果都一致，只是信心度有一點點差異（大約差 1-10%）。

#### 精度損失分析

在 Notebook 測試中，FP32 和 INT8 的輸出差異（relative L2 diff）只有 **0.6%**，這表示量化後的精度損失很小，幾乎不影響實際預測結果。

#### 小結

| 比較項目 | FP32 | INT8 | 結論 |
|----------|------|------|------|
| 檔案大小 | 43 MB | 11 MB | INT8 小 4 倍 |
| 推論速度 | 較慢 | 較快 | INT8 快 1~4 倍 |
| 預測結果 | - | - | 基本一致 |
| 精度損失 | - | 0.6% | 可以接受 |

量化的好處就是用很小的精度損失換來更小的模型和更快的速度，很適合部署到資源有限的設備上。

---

### 3.3 心得與建議

這次 Lab 讓我學到怎麼把訓練好的 PyTorch 模型轉換成 ONNX 格式，然後用量化技術壓縮模型。之前只知道量化可以讓模型變小變快，但沒有實際操作過，這次做完之後對整個流程比較有概念了。

Gradio 的部分我覺得蠻方便的，不用寫什麼前端程式碼就可以做出一個可以用的網頁介面，而且 `share=True` 可以直接產生公開網址讓手機連進來測試，這點還蠻神奇的。

比較可惜的是 CIFAR-10 的輸入只有 32x32，所以辨識一般的照片效果不是很好，尤其是那張貓咪玩偶根本看不出來是什麼。如果用更大的輸入尺寸或是用 ImageNet 預訓練的模型應該會好很多。

整體來說這次作業讓我對模型部署有更多的認識，以後如果要把模型放到手機或嵌入式設備上應該會比較知道要怎麼做。

---

## 繳交檔案清單

- [x] N26135011.ipynb
- [x] N26135011_FP32.onnx
- [x] N26135011_INT8.onnx
- [x] N26135011_report.pdf

