{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KRm0_WztAPe"
   },
   "source": [
    "# 2025 EAI Lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e7m9tHTpR0H"
   },
   "source": [
    "## Topic 1 : From PyTorch To ONNX\n",
    "\n",
    "### Steps:\n",
    "1.   Define Model Architecture\n",
    "2.   Load Weight\n",
    "3.   Export ONNX File\n",
    "4.   Quantize To INT8\n",
    "5.   Building Session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vt7LM45spK0Q"
   },
   "outputs": [],
   "source": [
    "# !pip install -U \\\n",
    "#     torch torchvision torchaudio \\\n",
    "#     onnx onnxscript onnxruntime onnxruntime-tools onnxruntime-gpu \\\n",
    "#     gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MCKZlTEIqkOD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TODO\n",
    "# Design Your ResNet18 Model\n",
    "# 參考 resnet18.py 來建立模型架構\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"ResNet 的基本區塊\"\"\"\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # 左邊的主要路徑：兩個 conv + bn，中間有 relu\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        \n",
    "        # shortcut：如果維度不一樣就要用 1x1 conv 調整\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 殘差連接：把輸入加回來\n",
    "        out = self.left(x)\n",
    "        out = out + self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    \"\"\"ResNet18 模型，用於 CIFAR-10 分類\"\"\"\n",
    "    def __init__(self, ResBlock, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        \n",
    "        # 第一層 conv\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 四個 layer，通道數分別是 64, 128, 256, 512\n",
    "        self.layer1 = self.make_layer(ResBlock, 64, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResBlock, 512, 2, stride=2)\n",
    "        \n",
    "        # 最後的全連接層，輸出 10 個類別\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        \"\"\"建立一個 layer，裡面有 num_blocks 個 block\"\"\"\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.inchannel, channels, s))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 依序通過各層\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)  # 平均池化\n",
    "        out = out.view(out.size(0), -1)  # 攤平成一維\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RUErZRINpUU5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wq/6rfqfffj6ns9plv8k0pyz2pm0000gn/T/ipykernel_28492/1720290952.py:28: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1205 15:46:41.452000 28492 site-packages/torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 13 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `ResNet18([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `ResNet18([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 13).\n",
      "Failed to convert the model to the target version 13 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eai_lab5/lib/python3.10/site-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eai_lab5/lib/python3.10/site-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eai_lab5/lib/python3.10/site-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eai_lab5/lib/python3.10/site-packages/onnx/version_converter.py\", line 39, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "RuntimeError: /Users/runner/work/onnx/onnx/onnx/version_converter/BaseConverter.h:68: adapter_lookup: Assertion `false` failed: No Adapter From Version $15 for Shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 41 of general pattern rewrite rules.\n",
      "⚠️ 模型被分割了，正在合併...\n",
      "✓ 合併完成: N26135011_FP32.onnx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch_model = ResNet18(ResBlock=BasicBlock, num_classes=10)\n",
    "dummy_input = (torch.randn(1, 3, 32, 32),)  # 假的輸入，用來讓 ONNX 知道輸入大小\n",
    "\n",
    "def export_onnx(model, dummy, path):\n",
    "  \"\"\"載入權重然後匯出成 ONNX\"\"\"\n",
    "  # 讀取老師給的預訓練權重\n",
    "  state = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "  # TODO : load state dict\n",
    "  # 這邊要過濾掉一些不需要的 key（像是 total_ops 這種分析用的）\n",
    "  model_state = {k: v for k, v in state.items() if not k.endswith(('total_ops', 'total_params'))}\n",
    "  model.load_state_dict(model_state, strict=False)\n",
    "\n",
    "  # 切換成 eval 模式，這樣 BN 層才會正常\n",
    "  torch_model.eval()\n",
    "\n",
    "  # Todo : Export ONNX FILE\n",
    "  import os\n",
    "  output_path = \"N26135011_FP32.onnx\"\n",
    "  \n",
    "  # 先把舊的檔案刪掉，避免出問題\n",
    "  if os.path.exists(output_path):\n",
    "      os.remove(output_path)\n",
    "  if os.path.exists(output_path + \".data\"):\n",
    "      os.remove(output_path + \".data\")\n",
    "  \n",
    "  # 匯出 ONNX 模型\n",
    "  torch.onnx.export(\n",
    "      model,                    # 我們的模型\n",
    "      dummy,                    # 假輸入 (1, 3, 32, 32)\n",
    "      output_path,              # 輸出檔名\n",
    "      export_params=True,       # 要匯出參數\n",
    "      opset_version=13,         # ONNX 版本\n",
    "      do_constant_folding=True, # 優化用的\n",
    "      input_names=[\"input\"],    # 輸入名稱\n",
    "      output_names=[\"output\"],  # 輸出名稱\n",
    "      dynamic_axes={            # 讓 batch size 可以變動\n",
    "          'input': {0: 'batch_size'},\n",
    "          'output': {0: 'batch_size'}\n",
    "      }\n",
    "  )\n",
    "  \n",
    "  # 如果模型太大被分割成兩個檔案，要合併回來\n",
    "  if os.path.exists(output_path + \".data\"):\n",
    "      print(f\"⚠️ 模型被分割了，正在合併...\")\n",
    "      import onnx\n",
    "      \n",
    "      # 載入然後重新存成單一檔案\n",
    "      onnx_model = onnx.load(output_path, load_external_data=True)\n",
    "      os.remove(output_path)\n",
    "      os.remove(output_path + \".data\")\n",
    "      onnx.save(onnx_model, output_path)\n",
    "      print(f\"✓ 合併完成: {output_path}\")\n",
    "  else:\n",
    "      print(f\"✓ FP32 模型匯出成功: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # 提醒 : 記得先把 best_model.pth 上傳到 Content 資料夾\n",
    "  export_onnx(model=torch_model, dummy=dummy_input, path=\"best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PnBfgSxfpUzD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calib will use input name: input\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "from PIL import Image\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import CalibrationDataReader\n",
    "\n",
    "CIFAR10_MEAN = np.array([0.4914, 0.4822, 0.4465], dtype=np.float32)\n",
    "CIFAR10_STD  = np.array([0.2470, 0.2435, 0.2616], dtype=np.float32)\n",
    "\n",
    "def preprocess_32x32(pil_img: Image.Image) -> np.ndarray:\n",
    "    arr = np.asarray(pil_img.convert(\"RGB\").resize((32, 32)), dtype=np.float32) / 255.0\n",
    "    arr = (arr - CIFAR10_MEAN) / CIFAR10_STD\n",
    "    return arr.transpose(2, 0, 1)[None, ...]  # (1,3,32,32)\n",
    "\n",
    "class CIFARLikeCalibReader(CalibrationDataReader):\n",
    "    def __init__(self, image_dir: str = None, input_name: str = \"input\",\n",
    "                 batch_size: int = 32, num_batches: int = 10):\n",
    "        self.input_name  = input_name\n",
    "        self.batch_size  = batch_size\n",
    "        self.num_batches = num_batches\n",
    "        self.paths = []\n",
    "        if image_dir and os.path.isdir(image_dir):\n",
    "            for f in os.listdir(image_dir):\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "                    self.paths.append(os.path.join(image_dir, f))\n",
    "        self._mode_random = len(self.paths) == 0\n",
    "        self._pos = 0\n",
    "        self._emitted = 0\n",
    "\n",
    "    def get_next(self):\n",
    "        if self._emitted >= self.num_batches:\n",
    "            return None\n",
    "        if self._mode_random:\n",
    "            batch = np.random.randn(self.batch_size, 3, 32, 32).astype(np.float32)\n",
    "        else:\n",
    "            items = []\n",
    "            for _ in range(self.batch_size):\n",
    "                if self._pos >= len(self.paths):\n",
    "                    break\n",
    "                img = Image.open(self.paths[self._pos])\n",
    "                self._pos += 1\n",
    "                items.append(preprocess_32x32(img))\n",
    "            if not items:\n",
    "                return None\n",
    "            batch = np.concatenate(items, axis=0).astype(np.float32)\n",
    "        self._emitted += 1\n",
    "        return {self.input_name: batch}\n",
    "\n",
    "    def rewind(self):\n",
    "        self._pos = 0\n",
    "        self._emitted = 0\n",
    "\n",
    "FP32_MODEL = \"N26135011_FP32.onnx\"\n",
    "INT8_MODEL = \"N26135011_INT8.onnx\"\n",
    "\n",
    "\n",
    "_tmp = ort.InferenceSession(FP32_MODEL, providers=[\"CPUExecutionProvider\"])\n",
    "INPUT_NAME = _tmp.get_inputs()[0].name\n",
    "print(\"Calib will use input name:\", INPUT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9_HL4D23phIN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please use QuantFormat.QDQ for activation type QInt8 and weight type QInt8. Or it will lead to bad performance on x64.\n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ INT8 模型量化完成: N26135011_INT8.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_static, QuantType, CalibrationMethod\n",
    "\n",
    "\n",
    "# 建立校準用的 reader（這邊用隨機資料來校準）\n",
    "reader = CIFARLikeCalibReader(\n",
    "    image_dir=None,          # 沒有圖片就用隨機資料\n",
    "    input_name=INPUT_NAME,   # 輸入的名稱\n",
    "    batch_size=1,            # 一次處理 1 張\n",
    "    num_batches=50           # 總共跑 50 次\n",
    ")\n",
    "\n",
    "\n",
    "def quantize_to_int8(fp32_path, int8_path, reader, method=\"MinMax\"):\n",
    "    \"\"\"把 FP32 模型量化成 INT8\"\"\"\n",
    "    # Todo : quantize_static\n",
    "    # 用 onnxruntime 的靜態量化功能\n",
    "    quantize_static(\n",
    "        model_input=fp32_path,                      # 輸入的 FP32 模型\n",
    "        model_output=int8_path,                     # 輸出的 INT8 模型\n",
    "        calibration_data_reader=reader,             # 校準用的 reader\n",
    "        quant_format=\"QOperator\",                   # 量化格式\n",
    "        per_channel=True,                           # per-channel 量化比較準\n",
    "        weight_type=QuantType.QInt8,                # 權重用 INT8\n",
    "        activation_type=QuantType.QInt8,            # activation 也用 INT8\n",
    "        calibrate_method=CalibrationMethod.MinMax   # 用 MinMax 方法校準\n",
    "    )\n",
    "    print(\"✓ INT8 模型量化完成:\", int8_path)\n",
    "\n",
    "quantize_to_int8(FP32_MODEL, INT8_MODEL, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VYZIE2Rdpj3G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Check] relative L2 diff FP32 vs INT8: 0.005177\n",
      "FP32 avg sec: 0.0069497632980346676\n",
      "INT8 avg sec: 0.0011230850219726563\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "def run(sess, x):\n",
    "    \"\"\"跑一次推論\"\"\"\n",
    "    return sess.run(None, {sess.get_inputs()[0].name: x})[0]\n",
    "\n",
    "x_demo = np.random.randn(1,3,32,32).astype(np.float32)  # 測試用的假資料\n",
    "\n",
    "# Todo : build session function\n",
    "def build_session(model_path, providers):\n",
    "  \"\"\"建立 ONNX Runtime 的 session 來跑推論\"\"\"\n",
    "  return ort.InferenceSession(model_path, providers=providers)\n",
    "\n",
    "\n",
    "\n",
    "sess_fp32 = build_session(model_path=FP32_MODEL, providers=[\"CPUExecutionProvider\"])\n",
    "sess_int8 = build_session(model_path=INT8_MODEL, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "y_fp32 = run(sess_fp32, x_demo)\n",
    "y_int8 = run(sess_int8, x_demo)\n",
    "\n",
    "l2_rel = np.linalg.norm(y_fp32 - y_int8) / (np.linalg.norm(y_fp32) + 1e-12)\n",
    "print(f\"[Check] relative L2 diff FP32 vs INT8: {l2_rel:.6f}\")\n",
    "\n",
    "def bench(sess, x, n=50):\n",
    "    t0 = time.time()\n",
    "    for _ in range(n):\n",
    "        sess.run(None, {sess.get_inputs()[0].name: x})\n",
    "    return (time.time() - t0) / n\n",
    "\n",
    "print(\"FP32 avg sec:\", bench(sess_fp32, x_demo))\n",
    "print(\"INT8 avg sec:\", bench(sess_int8, x_demo))\n",
    "\n",
    "so = ort.SessionOptions()\n",
    "so.enable_profiling = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeFccLG1pehx"
   },
   "source": [
    "## Topic 2 : Gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qFNo0K7gvJqd"
   },
   "outputs": [],
   "source": [
    "# ! pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_vE9Tel_tI76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/eai_lab5/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "當前工作目錄: /Users/jiaquan/Development/2025EAI_Project/EAI_Lab5\n",
      "FP32 模型: True - N26135011_FP32.onnx\n",
      "INT8 模型: True - N26135011_INT8.onnx\n",
      "可用的 providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 15:46:45.786 python[28492:2897037] 2025-12-05 15:46:45.785717 [W:onnxruntime:, coreml_execution_provider.cc:113 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 9 number of nodes in the graph: 148 number of nodes supported by CoreML: 9\n",
      "2025-12-05 15:46:46.218 python[28492:2897037] 2025-12-05 15:46:46.218599 [W:onnxruntime:, coreml_execution_provider.cc:113 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 2 number of nodes in the graph: 50 number of nodes supported by CoreML: 47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 兩個模型都載入成功了\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://a25cf08bab330a699c.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a25cf08bab330a699c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ====== 檢查環境 ======\n",
    "print(f\"當前工作目錄: {os.getcwd()}\")\n",
    "\n",
    "# ====== 設定 ======\n",
    "MODEL_PATH_INT8 = \"N26135011_INT8.onnx\"   # INT8 模型\n",
    "MODEL_PATH_FP32 = \"N26135011_FP32.onnx\"   # FP32 模型\n",
    "LABELS = ['plane','car','bird','cat','deer','dog','frog','horse','ship','truck']  # CIFAR-10 的 10 個類別\n",
    "\n",
    "# 檢查模型檔案有沒有存在\n",
    "print(f\"FP32 模型: {os.path.exists(MODEL_PATH_FP32)} - {MODEL_PATH_FP32}\")\n",
    "print(f\"INT8 模型: {os.path.exists(MODEL_PATH_INT8)} - {MODEL_PATH_INT8}\")\n",
    "if os.path.exists(MODEL_PATH_FP32 + '.data'):\n",
    "    print(f\"⚠️ 有 .data 檔案，請重跑 Cell 4\")\n",
    "\n",
    "# CIFAR-10 的正規化參數\n",
    "CIFAR10_MEAN = np.array([0.4914, 0.4822, 0.4465], dtype=np.float32)\n",
    "CIFAR10_STD  = np.array([0.2470, 0.2435, 0.2616], dtype=np.float32)\n",
    "\n",
    "# ====== 工具函數 ======\n",
    "def softmax_np(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"把 logits 轉成機率\"\"\"\n",
    "    x = x - np.max(x)\n",
    "    ex = np.exp(x)\n",
    "    return ex / np.sum(ex)\n",
    "\n",
    "# TODO : preprocess input image function\n",
    "def preprocess(image: Image.Image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    圖片前處理，把上傳的圖片轉成模型要的格式\n",
    "    輸入: PIL Image\n",
    "    輸出: (1, 3, 32, 32) 的 numpy array\n",
    "    \"\"\"\n",
    "    if not isinstance(image, Image.Image):\n",
    "        raise ValueError(\"請上傳圖片\")\n",
    "    \n",
    "    # 1. 轉成 RGB 然後縮放到 32x32\n",
    "    img = image.convert(\"RGB\").resize((32, 32))\n",
    "    \n",
    "    # 2. 轉成 numpy 並除以 255 變成 0~1\n",
    "    arr = np.asarray(img, dtype=np.float32) / 255.0\n",
    "    \n",
    "    # 3. 用 CIFAR-10 的 mean 和 std 做標準化\n",
    "    arr = (arr - CIFAR10_MEAN) / CIFAR10_STD\n",
    "    \n",
    "    # 4. 把 HWC 轉成 CHW（PyTorch 格式）\n",
    "    arr = arr.transpose(2, 0, 1)\n",
    "    \n",
    "    # 5. 加上 batch 維度變成 (1, 3, 32, 32)\n",
    "    arr = arr[None, ...]\n",
    "    \n",
    "    return arr\n",
    "\n",
    "# ====== 建立 Session ======\n",
    "def build_session(model_path, providers):\n",
    "    \"\"\"載入 ONNX 模型\"\"\"\n",
    "    return ort.InferenceSession(model_path, providers=providers)\n",
    "\n",
    "# ====== 載入模型 ======\n",
    "providers = ort.get_available_providers()\n",
    "print(f\"可用的 providers: {providers}\")\n",
    "\n",
    "sess_int8 = build_session(MODEL_PATH_INT8, providers=providers)\n",
    "in_int8  = sess_int8.get_inputs()[0].name\n",
    "out_int8 = sess_int8.get_outputs()[0].name\n",
    "\n",
    "# 載入 FP32 模型（用 try 包起來以防出錯）\n",
    "try:\n",
    "    sess_fp32 = build_session(MODEL_PATH_FP32, providers=providers)\n",
    "    in_fp32  = sess_fp32.get_inputs()[0].name\n",
    "    out_fp32 = sess_fp32.get_outputs()[0].name\n",
    "    _fp32_err = \"\"\n",
    "    print(\"✓ 兩個模型都載入成功了\")\n",
    "except Exception as e:\n",
    "    sess_fp32, in_fp32, out_fp32 = None, None, None\n",
    "    _fp32_err = f\"FP32 載入失敗: {e}\"\n",
    "    print(f\"✗ {_fp32_err}\")\n",
    "\n",
    "# ====== 比較 FP32 和 INT8 ======\n",
    "# TODO : Compare FP32 and INT8\n",
    "def compare_fp32_int8(image: Image.Image):\n",
    "    \"\"\"比較兩個模型的預測結果和速度\"\"\"\n",
    "    if image is None:\n",
    "        return {}, {}, \"請上傳圖片\"\n",
    "    if sess_fp32 is None:\n",
    "        return {}, {}, _fp32_err or \"FP32 模型沒載入成功\"\n",
    "\n",
    "    # 先做前處理\n",
    "    x = preprocess(image)\n",
    "\n",
    "    # Your progarm\n",
    "    # 跑 FP32 推論，順便計時\n",
    "    t0 = time.time()\n",
    "    result_fp32 = sess_fp32.run([out_fp32], {in_fp32: x})[0]\n",
    "    fp32_ms = (time.time() - t0) * 1000\n",
    "    \n",
    "    # 跑 INT8 推論，順便計時\n",
    "    t0 = time.time()\n",
    "    result_int8 = sess_int8.run([out_int8], {in_int8: x})[0]\n",
    "    int8_ms = (time.time() - t0) * 1000\n",
    "\n",
    "    # 用 softmax 轉成機率\n",
    "    p_fp32 = softmax_np(result_fp32[0])\n",
    "    p_int8 = softmax_np(result_int8[0])\n",
    "\n",
    "    def top3_map(p):\n",
    "        \"\"\"拿出前 3 名的預測\"\"\"\n",
    "        idx = np.argpartition(p, -3)[-3:]\n",
    "        idx = idx[np.argsort(p[idx])[::-1]]\n",
    "        return {LABELS[i]: float(p[i]) for i in idx}\n",
    "\n",
    "    top3_fp32 = top3_map(p_fp32)\n",
    "    top3_int8 = top3_map(p_int8)\n",
    "\n",
    "    # 整理結果\n",
    "    summary = (\n",
    "        f\"FP32 推論時間: {fp32_ms:.2f} ms\\n\"\n",
    "        f\"INT8 推論時間: {int8_ms:.2f} ms\\n\"\n",
    "        f\"加速比 (FP32/INT8): {(fp32_ms / max(int8_ms, 1e-9)):.2f}×\"\n",
    "    )\n",
    "    return top3_fp32, top3_int8, summary\n",
    "\n",
    "# ====== Gradio 介面 ======\n",
    "# TODO : Building GUI Interface\n",
    "demo = gr.Interface(\n",
    "    fn=compare_fp32_int8,           # 要跑的函數\n",
    "    inputs=gr.Image(type=\"pil\"),    # 輸入是圖片\n",
    "    outputs=[                       # 輸出有三個\n",
    "        gr.Label(label=\"FP32 Top-3\"),       # FP32 的前 3 名\n",
    "        gr.Label(label=\"INT8 Top-3\"),       # INT8 的前 3 名\n",
    "        gr.Textbox(label=\"效能比較\")         # 時間比較\n",
    "    ],\n",
    "    title=\"CIFAR-10 分類器 - FP32 vs INT8 比較\",\n",
    "    description=\"上傳圖片來比較 FP32 和 INT8 模型\\n支援：飛機、汽車、鳥、貓、鹿、狗、青蛙、馬、船、卡車\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # TODO : building a public web\n",
    "  # 啟動網頁，share=True 會產生公開網址\n",
    "  demo.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOV/rmsswwtq7Dm7IE3hnwD",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "eai_lab5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
