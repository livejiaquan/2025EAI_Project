{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIOuFyuT_AGR"
   },
   "source": [
    "## 載入雲端硬碟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCHAMnEe4zr_"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/your folder/ # 請改成自己的路徑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRESUP0gF6YS"
   },
   "source": [
    "## 安裝依賴項"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7XN1-uh1oZQ"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision timm==0.4.12 einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69kUiQw1Fe_o"
   },
   "source": [
    "## 下載checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fK0xZE6347ye"
   },
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwQTzUqs49lZ"
   },
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1FfvdEBss9f8gexjqEY8vOSUQe_8qwYuL -O evit_0.7.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGEsG_9EDIgo"
   },
   "source": [
    "## 基本配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgRNCpJFWaPF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6nQhGEqJT3Y"
   },
   "source": [
    "## MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oeo7Ovq3JOy3"
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(0) # 等於沒有dropout，因為 eval 時不會啟用 dropout，本lab只會做 eval，因此為求方便直接這樣設置\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_CT9znGJf8k"
   },
   "source": [
    "## Patch Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfOi_avhJrH0"
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.Identity()\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)             # (B, C, H/ps, W/ps)\n",
    "        x = x.flatten(2).transpose(1, 2)    # (B, N, C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c7aTCCRJyyC"
   },
   "source": [
    "##　Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-VeaGnAJ1wm"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, keep_rate=1.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(0)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(0)\n",
    "        self.keep_rate = keep_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape # N 有包含 CLS Token，若有 fused token 也會包括在內\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn) # [B, H, N, N]\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        left_tokens = N - 1\n",
    "        if self.keep_rate < 1 :\n",
    "            ########################################\n",
    "            # 請實作 EViT 的 token pruning  #\n",
    "            ########################################\n",
    "            # 1. 根據 keep_rate 設定要保留的 token 數\n",
    "            left_tokens =\n",
    "            # 2. image token (包括 fused token) 的 cls attention 分數\n",
    "\n",
    "            # 3. 按照分數取出 left_tokens 個分數高的 token 的 idx，並按照分數排序\n",
    "\n",
    "            # 4. 得到 index\n",
    "            index =\n",
    "            ########################################\n",
    "\n",
    "            return x, index, idx, cls_attn, left_tokens\n",
    "            # idx 是要保留的 token 的索引 [B, left_tokens]\n",
    "            # index 是 idx 擴展成多 channel 的版本 [B, left_tokens, C]\n",
    "            # cls_atten 是 cls token 對其他每個 token 的注意力\n",
    "            # left_tokens 是保留的 token 數量\n",
    "\n",
    "\n",
    "        return  x, None, None, None, left_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mnuMu83Kycz"
   },
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1_XeznCKzFH"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., keep_rate=1.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(dim, num_heads, keep_rate)\n",
    "        self.drop_path = nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.mlp = Mlp(dim, int(dim * mlp_ratio))\n",
    "        self.keep_rate = keep_rate\n",
    "\n",
    "    def forward(self, x, get_idx=False):\n",
    "        B, N, C = x.shape\n",
    "        x_norm = self.norm1(x)\n",
    "        tmp, index, idx, cls_attn, left_tokens =self.attn(x_norm)\n",
    "\n",
    "\n",
    "        x = x + self.drop_path(tmp)\n",
    "\n",
    "        if index is not None: # 代表這層 Transformer 需要 pruning\n",
    "            ########################################\n",
    "            # 請實作 EViT 的 token pruning  #\n",
    "            ########################################\n",
    "            # 1. 先得到要保留的 tokens\n",
    "\n",
    "            # 2. 再得到要融合的其他 tokens\n",
    "\n",
    "            # 3. 用 cls token 對每個要融合的 image tokens 的注意力作為權重，將所有要融合的 token 加權成一個 fused token。\n",
    "\n",
    "            # 4. 把 CLS Token, 要保留的 image tokens, fused token 三個 concat 在一起 # [B, left_tokens+2, C]\n",
    "            x =\n",
    "            ########################################\n",
    "\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        n_tokens = x.shape[1] - 1\n",
    "        if get_idx and index is not None:\n",
    "            return x, n_tokens, idx\n",
    "        return x, n_tokens, None\n",
    "\n",
    "####################################################\n",
    "#  提供一個取補集的 function 給同學可以使用  #\n",
    "####################################################\n",
    "def complement_idx(idx, dim):\n",
    "    \"\"\"\n",
    "    Compute the complement: set(range(dim)) - set(idx).\n",
    "    Args:\n",
    "        idx: input index [B, left_tokens]\n",
    "        dim: the max number of index for complement\n",
    "    \"\"\"\n",
    "    a = torch.arange(dim, device=idx.device)\n",
    "    ndim = idx.ndim\n",
    "    dims = idx.shape\n",
    "    n_idx = dims[-1]\n",
    "    dims = dims[:-1] + (-1, )\n",
    "    for i in range(1, ndim):\n",
    "        a = a.unsqueeze(0)\n",
    "    a = a.expand(*dims)\n",
    "    masked = torch.scatter(a, -1, idx, 0)\n",
    "    compl, _ = torch.sort(masked, dim=-1, descending=False)\n",
    "    compl = compl.permute(-1, *tuple(range(ndim - 1)))\n",
    "    compl = compl[n_idx:].permute(*(tuple(range(1, ndim)) + (0,)))\n",
    "    return compl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRw0I7_CLH9s"
   },
   "source": [
    "## 主體模型 EViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ju-SaTkNLIdN"
   },
   "outputs": [],
   "source": [
    "class EViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 num_classes=1000, embed_dim=384, depth=12,\n",
    "                 num_heads=6, mlp_ratio=4.0, keep_rate=(1, 1, 1, 0.7) + (1, 1, 0.7) + (1, 1, 0.7) + (1, 1)):\n",
    "        super().__init__()\n",
    "        self.keep_rate = keep_rate\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        self.pos_drop = nn.Dropout(0)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, keep_rate=keep_rate[i])\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.pre_logits = nn.Identity()\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"EViT\"\n",
    "\n",
    "    def forward_features(self, x, get_idx=False):\n",
    "        _, _, h, w = x.shape\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "\n",
    "        x = self.pos_drop(x + pos_embed)\n",
    "\n",
    "        left_tokens = []\n",
    "        idxs = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x, left_token, idx = blk(x, get_idx)\n",
    "            left_tokens.append(left_token)\n",
    "            if idx is not None:\n",
    "                idxs.append(idx)\n",
    "        x = self.norm(x)\n",
    "        return self.pre_logits(x[:, 0]), left_tokens, idxs\n",
    "\n",
    "\n",
    "    def forward(self, x, get_idx=False):\n",
    "        x, _, idxs = self.forward_features(x, get_idx)\n",
    "        x = self.head(x)\n",
    "        if get_idx:\n",
    "            return x, idxs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DOt98n0DANd"
   },
   "source": [
    "## 載入 EViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0rVW0jD9SvC"
   },
   "outputs": [],
   "source": [
    "model_path = './evit_0.7.pth'\n",
    "\n",
    "model = EViT(keep_rate=(1, 1, 1, 0.7) + (1, 1, 0.7) + (1, 1, 0.7) + (1, 1))\n",
    "checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHM3su5tDPoO"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xD7C96NWbIcy"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform_eval = transforms.Compose([\n",
    "    transforms.Resize(int((256 / 224) * 224), interpolation=3),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "dataset_val = datasets.ImageFolder(root='./imagenet/val', transform=transform_eval)\n",
    "# 新的 class_to_idx，把資料夾名稱轉成 int\n",
    "new_class_to_idx = {cls_name: int(cls_name) for cls_name in dataset_val.classes}\n",
    "# 舊索引所對應的資料夾名稱\n",
    "idx_to_class = {v: k for k, v in dataset_val.class_to_idx.items()}\n",
    "# 更新 samples 裡的 label\n",
    "dataset_val.samples = [\n",
    "    (path, new_class_to_idx[idx_to_class[old_label]])\n",
    "    for path, old_label in dataset_val.samples\n",
    "]\n",
    "# 更新 class_to_idx\n",
    "dataset_val.class_to_idx = new_class_to_idx\n",
    "\n",
    "val_loader = DataLoader(dataset_val, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1yqArjXDamS"
   },
   "source": [
    "## Visualize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYxWlaDQEZZ4"
   },
   "outputs": [],
   "source": [
    "from timm.utils import accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "\n",
    "def visualize_mask(data_loader, model, device):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    mean = torch.tensor(IMAGENET_DEFAULT_MEAN, device=device).reshape(3, 1, 1)\n",
    "    std = torch.tensor(IMAGENET_DEFAULT_STD, device=device).reshape(3, 1, 1)\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc1 = 0.0\n",
    "    total_acc5 = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    ii = 0\n",
    "    for i, (images, target) in enumerate(data_loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "        B = images.size(0)\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            output, idx = model(images, get_idx=True)\n",
    "            print('Predict: ',int(output.argmax(dim=1)))\n",
    "            print('Answer: ',int(target))\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        total_loss += loss.item() * B\n",
    "        total_acc1 += acc1.item() * B\n",
    "        total_acc5 += acc5.item() * B\n",
    "        total_samples += B\n",
    "\n",
    "        # denormalize\n",
    "        images = images * std + mean\n",
    "\n",
    "        idxs = get_real_idx(idx, fuse_token=True)\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(idxs) + 1, figsize=(4 * (len(idxs) + 1), 4))\n",
    "\n",
    "        # 原圖\n",
    "        axes[0].imshow(images[0].detach().cpu().permute(1, 2, 0).numpy().clip(0, 1))\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        # masked 圖\n",
    "        for jj, idx in enumerate(idxs):\n",
    "            masked_img = mask(images, patch_size=16, idx=idx)\n",
    "            img_np = masked_img[0].detach().cpu().permute(1, 2, 0).numpy().clip(0, 1)\n",
    "            axes[jj + 1].imshow(img_np)\n",
    "            axes[jj + 1].set_title(f\"Layer {jj*3+4}\")\n",
    "            axes[jj + 1].axis(\"off\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        ii += 1\n",
    "\n",
    "        print(f\"[{i+1}/{len(data_loader)}] \"\n",
    "                  f\"Loss: {total_loss / total_samples:.4f}, \"\n",
    "                  f\"Acc@1: {total_acc1 / total_samples:.2f}%, \"\n",
    "                  f\"Acc@5: {total_acc5 / total_samples:.2f}%\")\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc1 = total_acc1 / total_samples\n",
    "    avg_acc5 = total_acc5 / total_samples\n",
    "    print(f\"\\n* Final: Acc@1 {avg_acc1:.2f}%  Acc@5 {avg_acc5:.2f}%  Loss {avg_loss:.4f}\")\n",
    "\n",
    "    return {\"loss\": avg_loss, \"acc1\": avg_acc1, \"acc5\": avg_acc5}\n",
    "\n",
    "# 在多層 Transformer 裡，追蹤每一層被 prune 完保留的 token idx 對應到原始影像的實際位置。\n",
    "# 例如像第二層所保留的 token idx 是相對於第一層的結果，而不是相對於最一開始的輸入，因此要轉換回對應於最一開始的輸入 token\n",
    "def get_real_idx(idxs, fuse_token):\n",
    "    for i in range(1, len(idxs)):\n",
    "        ########################################\n",
    "        #       請實作         #\n",
    "        ########################################\n",
    "\n",
    "\n",
    "\n",
    "        idxs[i] =\n",
    "        ########################################\n",
    "\n",
    "    return idxs\n",
    "    # idxs 的每個元素都是有 pruning 的層要保留的 tokens 的索引(idx, [B, left_tokens])\n",
    "    # idxs[i] 是第 i 個有 pruning 的層的要保留的 tokens 的索引 [B, left_tokens]\n",
    "\n",
    "\n",
    "# 保留 idx 對應的 patch，其他位置全部填零（遮掉）\n",
    "def mask(x, idx, patch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: input image, shape: [B, 3, H, W]\n",
    "        idx: indices of masks, shape: [B, T]（T = 要保留的 token 數量）, value in range [0, h*w)\n",
    "    Return:\n",
    "        out_img: masked image with only patches from idx postions\n",
    "    \"\"\"\n",
    "    h = x.size(2) // patch_size\n",
    "    x = rearrange(x, 'b c (h p) (w q) -> b (c p q) (h w)', p=patch_size, q=patch_size)\n",
    "    output = torch.zeros_like(x)\n",
    "    idx1 = idx.unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "    extracted = torch.gather(x, dim=2, index=idx1)  # [b, c p q, T]\n",
    "    scattered = torch.scatter(output, dim=2, index=idx1, src=extracted)\n",
    "    out_img = rearrange(scattered, 'b (c p q) (h w) -> b c (h p) (w q)', p=patch_size, q=patch_size, h=h)\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5udZyKXDpVU"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPecsFa_FS49"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "visualize_mask(val_loader, model, device)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPRf97DRzUxFWDbcJudGawV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "EAI Lab1 (Python 3.9)",
   "language": "python",
   "name": "eai_lab1"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
