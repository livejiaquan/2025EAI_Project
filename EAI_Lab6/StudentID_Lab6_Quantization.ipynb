{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3239770b",
        "outputId": "43ed72d8-41e9-463a-f840-61d384276cac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAouVn-UqBbv"
      },
      "outputs": [],
      "source": [
        "import os as os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JCrxTYspYib"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/Your_Folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U4WMgPrpEMl"
      },
      "source": [
        "#â€‹ Block 1: Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gutVP_CahHat"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "#!pip install timm torch torchvision matplotlib numpy tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import timm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "from typing import Dict, List, Optional, Tuple, Union, Any\n",
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "print(f\"Timm version: {timm.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KUkT4Qpqpvy"
      },
      "source": [
        "# Block 2: Data Loading and Calibration Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaTVDx5bqrbi"
      },
      "outputs": [],
      "source": [
        "# CIFAR-10 data preparation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "\n",
        "# Create calibration subset (512 samples)\n",
        "calibration_indices = np.random.choice(len(trainset), 512, replace=False)\n",
        "calibration_set = Subset(trainset, calibration_indices)\n",
        "calibration_loader = DataLoader(calibration_set, batch_size=32, shuffle=False)\n",
        "\n",
        "# Test loader\n",
        "test_loader = DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(f\"Calibration samples: {len(calibration_set)}\")\n",
        "print(f\"Test samples: {len(testset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqEABynQq6jj"
      },
      "source": [
        "# Block 3: Load ViT-S Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq0VdVBJq9u1"
      },
      "outputs": [],
      "source": [
        "# Load pretrained ViT-S model\n",
        "model = timm.create_model('vit_small_patch16_224', pretrained=True, num_classes=10)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model: {model.__class__.__name__}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxWdtVPwrByZ"
      },
      "source": [
        "# Block 4: Fine-Tuning and Baseline Evaluation (UPDATED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtSsVAwKrC6a"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import os\n",
        "\n",
        "def train_model(model, train_loader, test_loader, device, epochs=10, lr=1e-4):\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for images, labels in pbar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100 * correct / total:.2f}%'\n",
        "            })\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save best model\n",
        "        if test_acc > best_accuracy:\n",
        "            best_accuracy = test_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'accuracy': test_acc,\n",
        "            }, './best_vit_cifar10.pth')\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "              f\"Train Loss: {train_loss:.4f}, \"\n",
        "              f\"Train Acc: {train_acc:.2f}%, \"\n",
        "              f\"Test Acc: {test_acc:.2f}%, \"\n",
        "              f\"Best Acc: {best_accuracy:.2f}%\")\n",
        "\n",
        "    return model, best_accuracy, history\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"Evaluate model accuracy\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    inference_time = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            start_time = time.time()\n",
        "            outputs = model(images)\n",
        "            inference_time += time.time() - start_time\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_latency = inference_time / len(dataloader)\n",
        "\n",
        "    return accuracy, avg_latency, inference_time\n",
        "\n",
        "# Fine-tune the model\n",
        "print(\"=\"*70)\n",
        "print(\"FINE-TUNING ViT-S ON CIFAR-10\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create training loader\n",
        "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "############### Write here #############\n",
        "# Fine-tune\n",
        "fine_tuned_model, best_acc, training_history = train_model(\n",
        "    model, train_loader, test_loader, device, epochs=, lr=\n",
        ")\n",
        "\n",
        "print(f\"\\nFine-tuning completed! Best accuracy: {best_acc:.2f}%\")\n",
        "print(f\"Best model saved to: ./best_vit_cifar10.pth\")\n",
        "\n",
        "# Load best model for evaluation\n",
        "checkpoint = torch.load('./best_vit_cifar10.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "print(\"\\nEvaluating fine-tuned FP32 baseline...\")\n",
        "fp32_accuracy, fp32_latency, fp32_time = evaluate_model(model, test_loader, device)\n",
        "print(f\"FP32 Fine-tuned - Accuracy: {fp32_accuracy:.2f}%, Avg Latency: {fp32_latency:.4f}s\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(training_history['train_loss'], label='Train Loss', marker='o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(training_history['train_acc'], label='Train Accuracy', marker='o')\n",
        "plt.plot(training_history['test_acc'], label='Test Accuracy', marker='s')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training Progress')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./training_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Training history plot saved to: ./training_history.png\")\n",
        "\n",
        "# Save final model with complete information\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'accuracy': fp32_accuracy,\n",
        "    'model_config': {\n",
        "        'name': 'vit_small_patch16_224',\n",
        "        'num_classes': 10,\n",
        "        'dataset': 'CIFAR-10'\n",
        "    },\n",
        "    'training_history': training_history\n",
        "}, './vit_s_cifar10_final.pth')\n",
        "\n",
        "print(f\"Final model saved to: ./vit_s_cifar10_final.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gA8nbab97SCU"
      },
      "outputs": [],
      "source": [
        "# Print model structure before smoothing\n",
        "print(\"=== Structure BEFORE Smoothing ===\")\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQvJbecfrNio"
      },
      "source": [
        "# Block 5: SmoothQuant Core - Activation Statistics Collection (UPDATED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7joa58QwrKIU"
      },
      "outputs": [],
      "source": [
        "class ActivationCollector:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.activations = {}\n",
        "        self.hooks = []\n",
        "\n",
        "    def register_hooks(self, model) -> None:\n",
        "        for name, module in model.named_modules():\n",
        "            # Write here\n",
        "            if isinstance(module, ): \n",
        "                hook = module.register_forward_hook(\n",
        "                    \n",
        "                )\n",
        "                self.hooks.append(hook)\n",
        "\n",
        "    def _get_hook(self, name):\n",
        "        def hook(modul: nn.Module, input: Tuple[torch.Tensor, ...], output:torch.Tensor):\n",
        "            # Record max absolute values per input channel\n",
        "            x = input[0].detach()\n",
        "\n",
        "            if x.dim() == 3:\n",
        "                x = x.reshape(-1, x.size(-1))\n",
        "            elif x.dim() == 2:\n",
        "                pass\n",
        "            else:\n",
        "                return\n",
        "\n",
        "            channel_max = torch.abs(x).max(dim=0)[0]\n",
        "\n",
        "            if name not in self.activations:\n",
        "                self.activations[name] = channel_max\n",
        "            else:\n",
        "                self.activations[name] = torch.max(\n",
        "                    self.activations[name], channel_max\n",
        "                )\n",
        "        return hook\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "        self.hooks = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k68Zg-UFtdD4"
      },
      "source": [
        "#Block 6: SmoothQuant - Compute Smoothing Factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYxWVN87tfe8"
      },
      "outputs": [],
      "source": [
        "def compute_smoothing_factors(model, activation_stats, alpha=0.5):\n",
        "\n",
        "    smoothing_factors = {}\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear) and name in activation_stats:\n",
        "\n",
        "            ############## Write here ##############\n",
        "            # 1. Get activation max per channel\n",
        "\n",
        "\n",
        "            # 2. Get weight max per input channel\n",
        "\n",
        "\n",
        "            # 3. Compute smoothing factor\n",
        "            s =\n",
        "\n",
        "            smoothing_factors[name] = s.to(device)\n",
        "\n",
        "    return smoothing_factors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tggSAl5TtHfL"
      },
      "source": [
        "#Block 7: SmoothQuant - Apply Smoothing Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o67BHCC5z9f1"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def smooth_ln_fcs(ln, fc, scales):\n",
        "\n",
        "    if not isinstance(fc, nn.Linear) or not isinstance(ln, (nn.LayerNorm, nn.Embedding)):\n",
        "        print(f\"Warning: Skipping smoothing, layers not match: {type(ln)}, {type(fc)}\")\n",
        "        return\n",
        "\n",
        "    ########### Write here ############\n",
        "    # 1. Smooth the Activation Source (LayerNorm)\n",
        "\n",
        "    # 2. Smooth the Weight (Linear)\n",
        "\n",
        "\n",
        "    # Note: Linear's bias does not need modification, because (X/s) * (W*s) + b = XW + b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmI-5o500Gxq"
      },
      "outputs": [],
      "source": [
        "def apply_smooth_quant_vit(model, smoothing_factors):\n",
        "\n",
        "    print(\"Applying SmoothQuant fusion (LN -> Linear)...\")\n",
        "\n",
        "    blocks = model.blocks if hasattr(model, 'blocks') else model.encoder.layers\n",
        "\n",
        "    for i, block in enumerate(blocks):\n",
        "\n",
        "        # Process Attention's QKV (Norm1 -> QKV)\n",
        "        qkv_key = f\"blocks.{i}.attn.qkv\"\n",
        "        if qkv_key in smoothing_factors:\n",
        "            scales = smoothing_factors[qkv_key]\n",
        "            # Pass LayerNorm (norm1) and Linear (qkv)\n",
        "            smooth_ln_fcs(block.norm1, block.attn.qkv, scales)\n",
        "\n",
        "        # Process FC1 (Norm2 -> FC1)\n",
        "        fc1_key = f\"blocks.{i}.mlp.fc1\"\n",
        "        if fc1_key in smoothing_factors:\n",
        "            scales = smoothing_factors[fc1_key]\n",
        "            # Pass LayerNorm (norm2) and Linear (fc1)\n",
        "            smooth_ln_fcs(block.norm2, block.mlp.fc1, scales)\n",
        "\n",
        "        # Skip FC2\n",
        "        # Because FC2 does not have LN before it\n",
        "\n",
        "    print(\"Fusion complete. LayerNorms and Linear weights have been updated.\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC_s5KfDtnrU"
      },
      "source": [
        "#Block 8: Quantization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59AfdzP65fF4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def quantize_weight_per_channel(w, n_bits=8):\n",
        "\n",
        "    ############ Write here ##############\n",
        "    # 1. calulate scales\n",
        "    scales =\n",
        "\n",
        "    # 2. claculate q_max\n",
        "    q_max =\n",
        "\n",
        "    # 3. clamp the scales\n",
        "    scales =\n",
        "\n",
        "    # 4. Quantize\n",
        "    t_quant =\n",
        "\n",
        "    # 5. Dequantize\n",
        "    w_dequant =\n",
        "\n",
        "    return w_dequant\n",
        "\n",
        "def quantize_activation_dynamic_per_token(t, n_bits=8):\n",
        "\n",
        "    ############ Write here ##############\n",
        "    # 1. calulate scales\n",
        "    scales =\n",
        "\n",
        "    # 2. claculate q_max\n",
        "    q_max =\n",
        "\n",
        "    # 3. clamp the scales\n",
        "    scales =\n",
        "\n",
        "    # 4. Quantize\n",
        "    t_quant =\n",
        "\n",
        "    # 5. Dequantize\n",
        "    t_dequant =\n",
        "\n",
        "    return t_dequant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSw_9hns7ppW"
      },
      "outputs": [],
      "source": [
        "class QuantizedLinear(nn.Module):\n",
        "    def __init__(self, original_layer, n_bits=8):\n",
        "\n",
        "        super().__init__()\n",
        "        self.in_features = original_layer.in_features\n",
        "        self.out_features = original_layer.out_features\n",
        "        self.n_bits = n_bits\n",
        "\n",
        "        # Weights are statically quantized\n",
        "        with torch.no_grad():\n",
        "            self.weight = nn.Parameter(\n",
        "                quantize_weight_per_channel(original_layer.weight.data, n_bits),\n",
        "                requires_grad=False\n",
        "            )\n",
        "\n",
        "            if original_layer.bias is not None:\n",
        "                self.bias = nn.Parameter(original_layer.bias.data.clone(), requires_grad=False)\n",
        "            else:\n",
        "                self.register_parameter('bias', None)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Call dynamic quantization function\n",
        "        x_q = quantize_activation_dynamic_per_token(x, self.n_bits)\n",
        "\n",
        "        return F.linear(x_q, self.weight, self.bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMc2bYFApNi1"
      },
      "outputs": [],
      "source": [
        "def quantize_model_dynamic(model, n_bits=8):\n",
        "    quantized_model = copy.deepcopy(model)\n",
        "    for name, module in quantized_model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "\n",
        "            quant_linear = QuantizedLinear(module, n_bits=n_bits)\n",
        "\n",
        "            parent_name = '.'.join(name.split('.')[:-1])\n",
        "            child_name = name.split('.')[-1]\n",
        "            if parent_name:\n",
        "                parent = quantized_model.get_submodule(parent_name)\n",
        "            else:\n",
        "                parent = quantized_model\n",
        "            setattr(parent, child_name, quant_linear)\n",
        "    return quantized_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHeGC5PSoLF6"
      },
      "source": [
        "# Block 9 : Application Place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jveCmBwKk-sb"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COLLECTING ACTIVATION STATISTICS (Fine-tuned Model)\")\n",
        "print(\"=\"*70)\n",
        "collector = ActivationCollector()\n",
        "collector.register_hooks(model)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, _ in tqdm(calibration_loader, desc=\"Calibration\"):\n",
        "        images = images.to(device)\n",
        "        _ = model(images)\n",
        "\n",
        "collector.remove_hooks()\n",
        "activation_stats = collector.activations\n",
        "print(f\"Collected statistics for {len(activation_stats)} linear layers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "77UWLRctLjlb"
      },
      "outputs": [],
      "source": [
        "smoothing_factors = compute_smoothing_factors(model, activation_stats, alpha=0.5)\n",
        "smoothed_model = copy.deepcopy(model)\n",
        "\n",
        "# Smooth\n",
        "print(\"Applying SmoothQuant Fusion...\")\n",
        "apply_smooth_quant_vit(smoothed_model, smoothing_factors)\n",
        "\n",
        "# Fake Quant\n",
        "print(\"Quantizing Model (Dynamic)...\")\n",
        "quantized_model = quantize_model_dynamic(smoothed_model, n_bits=8)\n",
        "quantized_model = quantized_model.to(device)\n",
        "\n",
        "print(\"Dynamic Quantization complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WxYwB8vD3EyP"
      },
      "outputs": [],
      "source": [
        "print(\"=== Structure After Smoothing and Quantize ===\")\n",
        "print(quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8VRuFcPtI85"
      },
      "source": [
        "# Block 9: Evaluate Quantized Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7djEOyHtrCQ"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATING QUANTIZED MODEL\")\n",
        "print(\"=\"*70)\n",
        "quant_accuracy, quant_latency, quant_time = evaluate_model(\n",
        "    quantized_model, test_loader, device\n",
        ")\n",
        "print(f\"W8A8 SmoothQuant - Accuracy: {quant_accuracy:.2f}%, Avg Latency: {quant_latency:.4f}s\")\n",
        "\n",
        "# Save quantized model\n",
        "torch.save({\n",
        "    'model_state_dict': quantized_model.state_dict(),\n",
        "    'smoothing_factors': smoothing_factors,\n",
        "    'alpha': alpha,\n",
        "    'accuracy': quant_accuracy,\n",
        "    'model_config': {\n",
        "        'name': 'vit_small_patch16_224',\n",
        "        'num_classes': 10,\n",
        "        'dataset': 'CIFAR-10',\n",
        "        'quantization': 'W8A8'\n",
        "    }\n",
        "}, './vit_s_quantized_cifar10.pth')\n",
        "print(f\"Quantized model saved to: ./vit_s_quantized_cifar10.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQiqO87GtN-P"
      },
      "source": [
        "# Block 10: Benchmark Results Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbkDhACTtvAv"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BENCHMARK RESULTS - CIFAR-10 (Fine-tuned Model)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Method':<30} {'Accuracy (%)':<15} {'Latency (s)':<15} {'Total Time (s)':<15}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'FP32 Fine-tuned':<30} {fp32_accuracy:<15.2f} {fp32_latency:<15.4f} {fp32_time:<15.2f}\")\n",
        "print(f\"{'W8A8 SmoothQuant':<30} {quant_accuracy:<15.2f} {quant_latency:<15.4f} {quant_time:<15.2f}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Accuracy Delta':<30} {quant_accuracy - fp32_accuracy:<15.2f}\")\n",
        "print(f\"{'Speedup':<30} {fp32_time / quant_time:<15.2f}x\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate memory savings\n",
        "fp32_size = sum(p.numel() * 4 for p in model.parameters()) / (1024**2)  # MB\n",
        "int8_size = sum(p.numel() for p in quantized_model.parameters()) / (1024**2)\n",
        "print(f\"\\nModel Size - FP32: {fp32_size:.2f} MB\")\n",
        "print(f\"Model Size - INT8: {int8_size:.2f} MB (approx)\")\n",
        "print(f\"Memory Reduction: {fp32_size / int8_size:.2f}x\")\n",
        "\n",
        "# Save benchmark results\n",
        "benchmark_results = {\n",
        "    'fp32': {'accuracy': fp32_accuracy, 'latency': fp32_latency, 'time': fp32_time},\n",
        "    'w8a8_smoothquant': {'accuracy': quant_accuracy, 'latency': quant_latency, 'time': quant_time},\n",
        "    'alpha': alpha,\n",
        "    'model_size_mb': {'fp32': fp32_size, 'int8': int8_size}\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('./benchmark_results.json', 'w') as f:\n",
        "    json.dump(benchmark_results, f, indent=4)\n",
        "\n",
        "print(f\"\\nBenchmark results saved to: ./benchmark_results.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v9YLw-qpd2-"
      },
      "source": [
        "# Block 11 : Comparison Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwU1qTvvEpdI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def plot_smoothquant_effect_fixed_scale(data_before, data_after, title_prefix, xlabel='Channel', ylabel='Token', sample_rows=60, sample_cols=60):\n",
        "\n",
        "    val_before = data_before.abs().detach().cpu().numpy()\n",
        "    val_after = data_after.abs().detach().cpu().numpy()\n",
        "\n",
        "    if val_before.ndim > 2: val_before = val_before[0]\n",
        "    if val_after.ndim > 2: val_after = val_after[0]\n",
        "\n",
        "    global_max = max(val_before.max(), val_after.max())\n",
        "    z_limit = global_max * 1.1\n",
        "\n",
        "    rows = min(val_before.shape[0], sample_rows)\n",
        "    cols = min(val_before.shape[1], sample_cols)\n",
        "\n",
        "    sub_before = val_before[:rows, :cols]\n",
        "    sub_after = val_after[:rows, :cols]\n",
        "\n",
        "    x = np.arange(cols)\n",
        "    y = np.arange(rows)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 9))\n",
        "\n",
        "    ax1 = fig.add_subplot(121, projection='3d')\n",
        "    ax1.plot_surface(X, Y, sub_before, cmap='viridis', edgecolor='none', alpha=0.9)\n",
        "    ax1.set_title(f\"{title_prefix}: Original\", fontsize=15, fontweight='bold')\n",
        "    ax1.set_xlabel(xlabel)\n",
        "    ax1.set_ylabel(ylabel)\n",
        "    ax1.set_zlabel('Magnitude')\n",
        "    ax1.set_zlim(0, z_limit)\n",
        "    ax1.view_init(elev=30, azim=-60)\n",
        "\n",
        "    ax2 = fig.add_subplot(122, projection='3d')\n",
        "    ax2.plot_surface(X, Y, sub_after, cmap='coolwarm', edgecolor='none', alpha=0.9)\n",
        "\n",
        "    if \"Weight\" in title_prefix:\n",
        "        desc = \"Smoothed (Result: Spikier)\"\n",
        "    else:\n",
        "        desc = \"Smoothed (Result: Flatter)\"\n",
        "\n",
        "    ax2.set_title(f\"{title_prefix}: {desc}\", fontsize=15, fontweight='bold')\n",
        "    ax2.set_xlabel(xlabel)\n",
        "    ax2.set_ylabel(ylabel)\n",
        "    ax2.set_zlabel('Magnitude')\n",
        "    ax2.set_zlim(0, z_limit)\n",
        "    ax2.view_init(elev=30, azim=-60)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# WEIGHTS: Original vs Smoothed\n",
        "print(\">>> PLOT: WEIGHT BEFORE/AFTER\")\n",
        "\n",
        "W_before = model.blocks[0].attn.qkv.weight.data\n",
        "W_after  = smoothed_model.blocks[0].attn.qkv.weight.data\n",
        "\n",
        "plot_smoothquant_effect_fixed_scale(\n",
        "    W_before, W_after,\n",
        "    \"Weights\",\n",
        "    xlabel='Input Channel', ylabel='Output Channel'\n",
        ")\n",
        "\n",
        "\n",
        "# ACTIVATIONS: Original vs Smoothed\n",
        "print(\"\\n>>> PLOT: ACTIVATION BEFORE/AFTER\")\n",
        "\n",
        "# different from ActivationCollector\n",
        "class DataCapturer:\n",
        "    def __init__(self):\n",
        "        self.data = None\n",
        "        self.handle = None\n",
        "    def hook_fn(self, m, i, o):\n",
        "        self.data = i[0].detach().cpu() # Capture Activation\n",
        "    def register(self, layer):\n",
        "        self.handle = layer.register_forward_hook(self.hook_fn)\n",
        "    def remove(self):\n",
        "        if self.handle: self.handle.remove()\n",
        "\n",
        "\n",
        "act_before = DataCapturer()\n",
        "act_after = DataCapturer()\n",
        "\n",
        "# Register\n",
        "act_before.register(model.blocks[0].attn.qkv)\n",
        "act_after.register(smoothed_model.blocks[0].attn.qkv)\n",
        "\n",
        "device = next(model.parameters()).device\n",
        "model.eval()\n",
        "smoothed_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img, _ in calibration_loader:\n",
        "        img = img.to(device)\n",
        "        model(img)          # Trigger act_before\n",
        "        smoothed_model(img) # Trigger act_after\n",
        "        break\n",
        "\n",
        "act_before.remove()\n",
        "act_after.remove()\n",
        "\n",
        "if act_before.data is not None and act_after.data is not None:\n",
        "    X_before = act_before.data[0]\n",
        "    X_after  = act_after.data[0]\n",
        "\n",
        "    print(f\"Activation Before Max: {X_before.abs().max():.2f}\")\n",
        "    print(f\"Activation After Max:  {X_after.abs().max():.2f}\")\n",
        "    print(\"(You should see the After Max is significantly smaller)\")\n",
        "\n",
        "    plot_smoothquant_effect_fixed_scale(\n",
        "        X_before, X_after,\n",
        "        \"Activations\",\n",
        "        xlabel='Channel', ylabel='Token'\n",
        "    )\n",
        "else:\n",
        "    print(\"Error: Failed to capture data.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
