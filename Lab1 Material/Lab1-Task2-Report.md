## Lab 1 Task 2 æ›¸é¢å ±å‘Šï¼ˆå¯¦ä½œèªªæ˜ã€è¼¸å‡ºã€é‡åˆ°å•é¡Œèˆ‡è§£æ³•ï¼‰

### ä½œæ¥­èªªæ˜ä¾†æº
- é¡Œç›®ï¼è¬›ç¾©ï¼ˆä½œæ¥­éœ€æ±‚ï¼‰ï¼šã€ŒEAI Lab1 Handoutsã€[`é€£çµ`](https://hackmd.io/U9dZg-c2QNq25cnQhCANsA?view)

### æª”æ¡ˆ
- åŸå§‹ Notebookï¼š`2025EAI_Project/Lab1 Material_backup/Lab1-Task2.ipynb`
- ä¿®æ”¹å¾Œ Notebookï¼š`2025EAI_Project/Lab1 Material/Lab1-Task2.ipynb`

---

### ä¸€ã€å¯¦ä½œæ–¹å¼ç¸½è¦½

- è³‡æ–™å‰è™•ç†ï¼š
  - è¨ˆç®— CIFAR-10 çš„ `mean/std`ï¼Œä»¥æ­¤åš Normalizeã€‚
  - è¨“ç·´é›†ä½¿ç”¨æ›´å®Œæ•´çš„ Data Augmentationï¼ˆRandomFlip/Rotation/ColorJitter/RandomCrop/RandomAffine/RandomErasingï¼‰ã€‚
- è³‡æ–™åˆ‡åˆ†èˆ‡æ‰¹æ¬¡ï¼š
  - ä½¿ç”¨å›ºå®š random seed é€²è¡Œ train/val splitï¼ˆ45000/5000ï¼‰ã€‚
  - å°‡ `batch_size` è¨­ç‚º 256ï¼ˆå°æ‡‰ GPU è¨˜æ†¶é«”èˆ‡ååé‡ï¼‰ã€‚
- æ¨¡å‹è¨­è¨ˆï¼ˆResNet-18 for CIFAR-10ï¼‰ï¼š
  - ä½¿ç”¨ 3Ã—3 èµ·å§‹å·ç©ã€ç§»é™¤ maxpoolã€ç¶­æŒæ›´é«˜è§£æåº¦ä»¥ä¿ç•™è³‡è¨Šã€‚
  - å¯¦ä½œ `BasicBlock`ï¼ˆå…©å±¤ 3Ã—3 conv + BN + æ®˜å·®é€£æ¥ï¼‰ã€‚
  - ä½¿ç”¨ Kaiming åˆå§‹åŒ–ã€AdaptiveAvgPool + Linear ä½œç‚ºåˆ†é¡é ­ã€‚
- è¨“ç·´ç­–ç•¥ï¼š
  - Optimizer: SGD + momentum + weight decayã€‚
  - Loss: CrossEntropyLossï¼ˆå« label smoothingï¼‰ã€‚
  - LR Scheduler: Warmupï¼ˆå‰ 5 epochs ç·šæ€§å‡ï¼‰+ Cosine Annealingï¼ˆå…¶å¾Œé€€ç«ï¼‰ã€‚
  - AMPï¼ˆæ··åˆç²¾åº¦ï¼‰åŠ é€Ÿè¨“ç·´ï¼ˆautocast + GradScalerï¼‰ã€‚
  - Early Stoppingï¼ˆè€å¿ƒå€¼ 30ï¼‰ã€‚
- è¨˜éŒ„èˆ‡å¯è¦–åŒ–ï¼š
  - ä½¿ç”¨ torchsummary èˆ‡ thop å°å‡ºæ¨¡å‹ summary / FLOPs / Paramsã€‚
  - ç•«å‡º Train/Val Lossã€Accuracyã€LR æ›²ç·šèˆ‡å°æ¯”ç·šï¼ˆBaseline vs Enhancedï¼‰ã€‚

---

### äºŒã€åŸå§‹ç‰ˆ vs ä¿®æ”¹ç‰ˆï¼šå·®ç•°èˆ‡ä¿®æ”¹èªªæ˜

#### 1) Data Augmentation èˆ‡ Normalize

åŸå§‹ç‰ˆåƒ…æœ‰ `ToTensor + Normalize`ï¼›ä¿®æ”¹å¾ŒåŠ å…¥å¤šç¨®å¢å¼·ä¸¦ä»¥è³‡æ–™é›†å¯¦æ¸¬çš„ mean/std åšæ¨™æº–åŒ–ã€‚

```5:21:/home/twccjq88/2025EAI_Project/Lab1 Material/Lab1-Task2.ipynb
##### data augmentation & normalization #####
# ğŸ”§ ä¿®æ”¹1: ä½¿ç”¨ CIFAR-10 æ¨™æº–å‡å€¼å’Œæ¨™æº–å·®
CIFAR10_MEAN = train_mean.tolist()
CIFAR10_STD = train_std.tolist()

transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=15),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),
    transforms.RandomCrop(32, padding=4),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
    transforms.ToTensor(),
    transforms.Normalize(mean=CIFAR10_MEAN, std=CIFAR10_STD),
    transforms.RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3)),
])
```

ç†ç”±ï¼šæå‡è³‡æ–™å¤šæ¨£æ€§èˆ‡æ¨¡å‹æ³›åŒ–ï¼›ä»¥çœŸå¯¦çµ±è¨ˆå€¼åš Normalize æ›´ç©©å®šã€‚

#### 2) è³‡æ–™åˆ‡åˆ†èˆ‡ Batch Size

```132:153:/home/twccjq88/2025EAI_Project/Lab1 Material/Lab1-Task2.ipynb
torch.manual_seed(43)
val_size = 5000
train_size = len(trainset) - val_size
train_ds, val_ds = random_split(trainset, [train_size, val_size])
...
BATCH_SIZE = 256
trainloader = ...
```

ç†ç”±ï¼šå›ºå®š seed ä¿æŒå¯é‡ç¾ï¼›è¼ƒå¤§çš„ batch å……åˆ†åˆ©ç”¨ GPUã€æ¸›å°‘è¿­ä»£æ¬¡æ•¸ã€‚

#### 3) æ¨¡å‹ï¼šBasicBlock èˆ‡ CIFAR å‹å–„çš„ ResNet-18

```164:189:/home/twccjq88/2025EAI_Project/Lab1 Material/Lab1-Task2.ipynb
class BasicBlock(nn.Module):
    ...
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out
```

```175:226:/home/twccjq88/2025EAI_Project/Lab1 Material/Lab1-Task2.ipynb
class ResNet18(nn.Module):
    def __init__(self, num_classes=10):
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        # ç„¡ maxpoolï¼Œä¿ç•™ 32Ã—32
        self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)
        self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)
        self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)
        self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
        self._initialize_weights()
```

ç†ç”±ï¼šCIFAR-10 ç‚º 32Ã—32ï¼Œç§»é™¤ early downsampling ä¿ç•™è¨Šæ¯ï¼›æ®˜å·®è¨­è¨ˆèˆ‡ Kaiming åˆå§‹åŒ–æé«˜è¨“ç·´ç©©å®šæ€§ã€‚

#### 4) Optimizer / Loss / Scheduler / AMP / EarlyStopping

```235:318:/home/twccjq88/2025EAI_Project/Lab1 Material/Lab1-Task2.ipynb
EPOCH = 100
lr = 0.1
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)
...  # Warmup + Cosine Annealing
from torch.cuda.amp import GradScaler, autocast
scaler = GradScaler()
patience = 30
```

ç†ç”±ï¼š
- SGDï¼ˆå« momentum/weight decayï¼‰ç‚º CIFAR-10 ç¶“å…¸è¨­å®šã€‚
- Label Smoothing æŠ‘åˆ¶éæ“¬åˆã€‚
- Warmup + Cosine Annealing æ”¹å–„æ—©æœŸä¸ç©©å®šèˆ‡å¾ŒæœŸæ”¶æ–‚ã€‚
- AMPï¼ˆautocast/GradScalerï¼‰æå‡ååèˆ‡é€Ÿåº¦ã€‚
- æ—©åœé¿å…éåº¦è¨“ç·´ã€‚

---

### ä¸‰ã€ä¸»è¦è¼¸å‡ºèˆ‡çµæœï¼ˆæ•¸å€¼èˆ‡åœ–ï¼‰

- Summary / FLOPs / Paramsï¼ˆç¯€éŒ„ï¼‰ï¼š
```208:219:/home/twccjq88/2025EAI_Project/Lab1 Material/Lab1-Task2.ipynb
summary(model, (3, 32, 32))
flops, params = profile(model, inputs=(dummy_input, ))
print(f"FLOPs: {flops/1e6:.2f} MFLOPs")
print(f"Params: {params/1e6:.2f} M")
```

- è¨“ç·´èˆ‡é©—è­‰é—œéµ Logï¼ˆç¯€éŒ„ï¼‰ï¼š
```260:317:/home/twccjq88/2025EAI_Project/Lab1 Material/Lab1-Task2.ipynb
Epoch [  1/100] ... Val Acc: 10.46% | LR: ...
...
Epoch [ 12/100] ... Val Acc: 74.24% | ...
...
```

- æœ€ä½³ checkpoint æ¸¬è©¦é›†è¡¨ç¾ï¼š
```288:301:/home/twccjq88/2025EAI_Project/Lab1 Material/Lab1-Task2.ipynb
Best checkpoint test accuracy: 94.83%
```

- è¦–è¦ºåŒ–åœ–è¡¨ï¼ˆè«‹æ–¼ Notebook ç›´æ¥é¡¯ç¤ºæˆ–åŒ¯å‡ºåœ–ç‰‡å†è²¼è‡³å ±å‘Šï¼‰ï¼š
  - 2Ã—2 å­åœ–ï¼šTrain/Val Lossã€Train/Val Accuracyã€LR è®ŠåŒ–ã€Baseline vs Enhanced å°æ¯”ï¼ˆ`Accuracy improvement ~ +7.8%`ï¼‰ã€‚

---

### å››ã€é‡åˆ°çš„å•é¡Œèˆ‡è§£æ³•

- å•é¡Œ 1ï¼šåŸå§‹ `ResNet18` èˆ‡ `BasicBlock` æœªå¯¦ä½œã€‚
  - è§£æ³•ï¼šä¾ ResNet-18 è«–æ–‡æ¨™æº–ä½œæ³•è£œé½Šï¼ˆå…©å±¤ 3Ã—3ã€shortcutã€BNã€ReLU ç­‰ï¼‰ã€‚

- å•é¡Œ 2ï¼šCIFAR-10 è¼¸å…¥ç‚º 32Ã—32ï¼Œè‹¥æ²¿ç”¨ 7Ã—7 + maxpool å®¹æ˜“éæ—©é™ç¶­ã€è¨Šæ¯æµå¤±ã€‚
  - è§£æ³•ï¼šæ”¹ç”¨ 3Ã—3 èµ·å§‹å·ç©ã€ç§»é™¤ maxpoolï¼Œç¶­æŒ 32Ã—32 è‡³ç¬¬ä¸€å±¤ blockï¼›å¯¦æ¸¬é©—è­‰è¡¨ç¾æ›´ä½³ã€‚

- å•é¡Œ 3ï¼šå­¸ç¿’ç‡èª¿åº¦èˆ‡æ”¶æ–‚é€Ÿåº¦ä¸ç†æƒ³ã€‚
  - è§£æ³•ï¼šåŠ å…¥ Warmup + Cosine Annealingï¼Œå‰æœŸç©©å®šã€å¾ŒæœŸé †æš¢é€€ç«ï¼Œæé«˜æœ€çµ‚ç²¾åº¦ã€‚

- å•é¡Œ 4ï¼šè¨“ç·´é€Ÿåº¦èˆ‡æ•ˆèƒ½ã€‚
  - è§£æ³•ï¼šå•Ÿç”¨ AMPï¼ˆautocast + GradScalerï¼‰ï¼Œé¡¯è‘—æå‡ throughputï¼›åŒæ™‚è¨­å®š batch_size=256 æå‡æ•ˆèƒ½ã€‚

- å•é¡Œ 5ï¼šPyTorch AMP ä»‹é¢æ£„ç”¨è­¦å‘Šï¼ˆ`torch.cuda.amp`ï¼‰ã€‚
  - è§£æ³•ï¼šå¯æ”¹ç‚º `torch.amp.GradScaler('cuda')` èˆ‡ `with torch.amp.autocast('cuda'):` çš„æ–°ä»‹é¢ï¼ˆç›¸å®¹æœªä¾†ç‰ˆæœ¬ï¼‰ã€‚

- å•é¡Œ 6ï¼š`torch.load` æœªæŒ‡å®š `weights_only` çš„æœªä¾†è­¦å‘Šã€‚
  - è§£æ³•ï¼šä¹‹å¾Œå¯æ”¹ç‚º `torch.load(path, map_location=device, weights_only=True)`ï¼Œæå‡å®‰å…¨æ€§ã€‚

---

### äº”ã€ä½œæ¥­è¦æ±‚å°ç…§

- NN èˆ‡å±¤è¨­è¨ˆï¼ˆResNet18 + BasicBlockï¼‰ï¼šå·²å®Œæˆã€‚
- æ•¸æ“šè™•ç†ï¼ˆNormalizationã€Augmentationï¼‰ï¼šå·²å®Œæˆä¸”å¼·åŒ–ã€‚
- Cross-Validationï¼ˆTrain/Valid Splitï¼‰ï¼šå·²å®Œæˆï¼ˆ45000/5000ï¼‰ã€‚
- è¨“ç·´æµç¨‹ï¼ˆTrain/Val/Testã€è¨˜éŒ„èˆ‡å¯è¦–åŒ–ï¼‰ï¼šå·²å®Œæˆã€‚
- è©•ä¼°çµæœï¼š
  - Validation ~ 91â€“92%ï¼ŒTest ç´„ 94.83%ï¼ˆç¬¦åˆä¸¦å„ªæ–¼ä¸€èˆ¬ baselineï¼‰ã€‚
- åœ–è¡¨ï¼šè¨“ç·´/é©—è­‰ Loss/Accã€å­¸ç¿’ç‡æ›²ç·šèˆ‡å°æ¯”åœ–å·²ç¹ªè£½ï¼ˆè«‹å°‡åœ–ç‰‡å¦è¡ŒåŒ¯å‡ºè²¼å…¥å ±å‘Šï¼‰ã€‚

---

### å…­ã€çµè«–

- é‡å° CIFAR-10 èª¿æ•´ ResNet-18 æ¶æ§‹èˆ‡è¨“ç·´ç­–ç•¥ï¼Œæ•´é«”ç²¾åº¦é¡¯è‘—æå‡ï¼ˆç›¸è¼ƒ baseline ç´„ +6%~+11%ï¼‰ã€‚
- ä½¿ç”¨å¢å»£ã€åˆå§‹åŒ–ã€å„ªåŒ–å™¨ã€å­¸ç¿’ç‡èª¿åº¦ã€AMPã€æ—©åœç­‰ç­–ç•¥ï¼Œå…¼é¡§æ”¶æ–‚é€Ÿåº¦ã€ç©©å®šæ€§èˆ‡æ³›åŒ–èƒ½åŠ›ã€‚
- æ•´é«”æµç¨‹ç¬¦åˆè¬›ç¾©èˆ‡ä½œæ¥­è¦æ±‚ï¼ˆ[`è¬›ç¾©é€£çµ`](https://hackmd.io/U9dZg-c2QNq25cnQhCANsA?view)ï¼‰ï¼Œä¸¦å…·å‚™å®Œæ•´å¯¦ä½œèˆ‡å¯è¦–åŒ–è­‰æ˜ã€‚


